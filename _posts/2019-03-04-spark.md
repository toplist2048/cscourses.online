---
ID: 682
post_title: Spark
author: admin
post_excerpt: ""
layout: post
permalink: https://cscourses.online/topics/spark/
published: true
post_date: 2019-03-04 05:28:12
---
<meta http-equiv="content-type" content="text/html; charset=gbk">
<h2>Spark Introduction</h2>
Apache Spark is a cluster computing platform.

Spark provides high-level operators to build parallel apps. It has interactive Scala, Python, R, and SQL shells.

Spark stack including SQL, DataFrames, MLlib for machine learning, GraphX, and Spark Streaming.&nbsp;

Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud.

Spark is written in Scala, and runs on Java Virtual Machine (JVM). Spark support API from Python, Java, or Scala.
<h3>Spark Installation</h3>
You can download Spark from <a href="https://spark.apache.org/">Spark Official Site</a>, or you can install it by run,
<pre>pip install pyspark</pre>
<h3>Spark Shell</h3>
To open the Python version of the Spark shell, or the PySpark Shell, type:
<pre>pyspark</pre>
<h4>SparkContext</h4>
Spark application consists of a driver program that launches various parallel operations on a cluster. The driver program contains your application's main function and defines distributed dataset on the cluster, then applies operations to them. Spark shell is a driver program.

Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster. In the shell, a SparkContext is automatically created for you as the variable called sc.
<pre>&gt;&gt;&gt; sc
&lt;SparkContext master=local[*] appName=PySparkShell&gt;
&gt;&gt;&gt;    
</pre>
<h3>Spark Standalone Application</h3>
Apart from running interactively, Spark can be linked into standalone applications in either Java, Scala, or Python.

In Python, you can write applications as Python scripts, and run them using the spark-submit script included in Spark.
<h2>Spark RDDs</h2>
An Spark RDD is an immutable distributed collection of objects. Each RDD is split into multiple partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala objects.

In Spark all work is expressed as creating RDDs from external data; transforming them to define new RDDs using transformations like filter(); Caching by persist(); Launching actions such as count() and first() to kick off a parallel computation.